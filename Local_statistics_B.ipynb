{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-13T11:39:54.638064100Z",
     "start_time": "2023-08-13T11:39:54.620519800Z"
    }
   },
   "id": "2289c187"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data_path = \"./ex_all.csv\"\n",
    "custom_palette = {\n",
    "    \"ER\": \"#1f77b4\",      # Blue color\n",
    "    \"BER\": \"#ff7f0e\",     # Orange color\n",
    "    \"PER\": \"#2ca02c\",     # Green color\n",
    "    \"PBER\": \"#d62728\"     # Red color\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-13T11:39:54.638064100Z",
     "start_time": "2023-08-13T11:39:54.624028100Z"
    }
   },
   "id": "34a7d36c9ad25b7f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.6828636610867 72.67446387092832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2163/1944421607.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df_filtered[\"observe\"] = data_df_filtered[\"all\"] - data_df_filtered[[\"store\", \"sample\", \"train\", \"update\"]].sum(axis=1)\n",
      "/tmp/ipykernel_2163/1944421607.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df_filtered[\"num_env_steps_sampled\"] = data_df_filtered[\"num_env_steps_sampled\"] * 4\n",
      "/tmp/ipykernel_2163/1944421607.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df_filtered[\"sample_rate\"] = data_df_filtered[\"num_env_steps_sampled\"] / data_df_filtered[\"time_total_s\"]\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv(data_path)\n",
    "data_df = data_df.copy()\n",
    "data_df_filtered = data_df[data_df[\"num_env_steps_sampled\"] <= 400000]\n",
    "data_df_filtered[\"observe\"] = data_df_filtered[\"all\"] - data_df_filtered[[\"store\", \"sample\", \"train\", \"update\"]].sum(axis=1)\n",
    "\n",
    "# 2. Calculate the mean sample_rate for each environment and buffer type\n",
    "data_df_filtered[\"num_env_steps_sampled\"] = data_df_filtered[\"num_env_steps_sampled\"] * 4\n",
    "data_df_filtered[\"sample_rate\"] = data_df_filtered[\"num_env_steps_sampled\"] / data_df_filtered[\"time_total_s\"]\n",
    "mean_sample_rates = data_df_filtered.groupby(['env', 'buffer'])[\"sample_rate\"].mean().reset_index()\n",
    "\n",
    "# 3. Calculate the percentage difference between ER and BER\n",
    "per_sample_rates = mean_sample_rates[mean_sample_rates['buffer'] == 'PER'][\"sample_rate\"].values\n",
    "pber_sample_rates = mean_sample_rates[mean_sample_rates['buffer'] == 'PBER'][\"sample_rate\"].values\n",
    "percentage_differences = ((pber_sample_rates - per_sample_rates) / per_sample_rates) * 100\n",
    "\n",
    "# 4. Calculate the average percentage difference across all environments\n",
    "average_percentage_difference = percentage_differences.mean()\n",
    "\n",
    "min_max_sampled_per_pber = data_df_filtered[data_df_filtered['buffer'].isin(['PER', 'PBER'])].groupby(['env', 'buffer'])['num_env_steps_sampled'].max().unstack().min(axis=1)\n",
    "min_max_sampled_ber_er = data_df_filtered[data_df_filtered['buffer'].isin(['BER', 'ER'])].groupby(['env', 'buffer'])['num_env_steps_sampled'].max().unstack().min(axis=1)\n",
    "\n",
    "data_df_filtered = data_df_filtered.merge(min_max_sampled_per_pber.reset_index(), on='env', how='left')\n",
    "data_df_filtered.rename(columns={0: 'num_env_steps_sampled_min_max_for_per_pber'}, inplace=True)\n",
    "\n",
    "data_df_filtered = data_df_filtered.merge(min_max_sampled_ber_er.reset_index(), on='env', how='left')\n",
    "data_df_filtered.rename(columns={0: 'num_env_steps_sampled_min_max_for_ber_er'}, inplace=True)\n",
    "\n",
    "filtered_data_per_pber = data_df_filtered[(data_df_filtered['num_env_steps_sampled'] <= data_df_filtered['num_env_steps_sampled_min_max_for_per_pber']) & data_df_filtered['buffer'].isin(['PBER', 'PER'])]\n",
    "# 5. Calculate the total operation time for each buffer type\n",
    "columns_to_compare = [\"store\", \"sample\", \"update\"]\n",
    "\n",
    "per_total_time = filtered_data_per_pber[filtered_data_per_pber['buffer'] == 'PER'][columns_to_compare].sum().sum()\n",
    "pber_total_time = filtered_data_per_pber[filtered_data_per_pber['buffer'] == 'PBER'][columns_to_compare].sum().sum()\n",
    "\n",
    "# 6. Calculate the time difference between PER and PBER\n",
    "time_saved = per_total_time - pber_total_time\n",
    "percentage_time_saved = (time_saved / per_total_time) * 100\n",
    "print(average_percentage_difference, percentage_time_saved)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-13T11:39:54.784118700Z",
     "start_time": "2023-08-13T11:39:54.634032900Z"
    }
   },
   "id": "2aa098f695fc4273"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "env\nBeamRider        1600000\nBreakout         1600000\nQbert            1600000\nSpaceInvaders    1600000\ndtype: int64"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_sampled_per_pber"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-13T11:39:54.791963200Z",
     "start_time": "2023-08-13T11:39:54.783119400Z"
    }
   },
   "id": "186316f9a4536d7b"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2163/3549563666.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  env_data[\"time_interval\"] = pd.cut(env_data[\"time(h)\"], bins, labels=bins[:-1], right=False)\n",
      "/tmp/ipykernel_2163/3549563666.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  env_data[\"time_interval\"] = pd.cut(env_data[\"time(h)\"], bins, labels=bins[:-1], right=False)\n",
      "/tmp/ipykernel_2163/3549563666.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  env_data[\"time_interval\"] = pd.cut(env_data[\"time(h)\"], bins, labels=bins[:-1], right=False)\n",
      "/tmp/ipykernel_2163/3549563666.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  env_data[\"time_interval\"] = pd.cut(env_data[\"time(h)\"], bins, labels=bins[:-1], right=False)\n",
      "/tmp/ipykernel_2163/3549563666.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  env_data[\"time_interval\"] = pd.cut(env_data[\"time(h)\"], bins, labels=bins[:-1], right=False)\n",
      "/tmp/ipykernel_2163/3549563666.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  env_data[\"time_interval\"] = pd.cut(env_data[\"time(h)\"], bins, labels=bins[:-1], right=False)\n",
      "/tmp/ipykernel_2163/3549563666.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  env_data[\"time_interval\"] = pd.cut(env_data[\"time(h)\"], bins, labels=bins[:-1], right=False)\n",
      "/tmp/ipykernel_2163/3549563666.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  env_data[\"time_interval\"] = pd.cut(env_data[\"time(h)\"], bins, labels=bins[:-1], right=False)\n",
      "/tmp/ipykernel_2163/3549563666.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  env_data[\"time_interval\"] = pd.cut(env_data[\"time(h)\"], bins, labels=bins[:-1], right=False)\n",
      "/tmp/ipykernel_2163/3549563666.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  env_data[\"time_interval\"] = pd.cut(env_data[\"time(h)\"], bins, labels=bins[:-1], right=False)\n",
      "/tmp/ipykernel_2163/3549563666.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  env_data[\"time_interval\"] = pd.cut(env_data[\"time(h)\"], bins, labels=bins[:-1], right=False)\n",
      "/tmp/ipykernel_2163/3549563666.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  env_data[\"time_interval\"] = pd.cut(env_data[\"time(h)\"], bins, labels=bins[:-1], right=False)\n",
      "/tmp/ipykernel_2163/3549563666.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  env_data[\"time_interval\"] = pd.cut(env_data[\"time(h)\"], bins, labels=bins[:-1], right=False)\n",
      "/tmp/ipykernel_2163/3549563666.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  env_data[\"time_interval\"] = pd.cut(env_data[\"time(h)\"], bins, labels=bins[:-1], right=False)\n",
      "/tmp/ipykernel_2163/3549563666.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  env_data[\"time_interval\"] = pd.cut(env_data[\"time(h)\"], bins, labels=bins[:-1], right=False)\n",
      "/tmp/ipykernel_2163/3549563666.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  env_data[\"time_interval\"] = pd.cut(env_data[\"time(h)\"], bins, labels=bins[:-1], right=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": "               R_improve_ER_BER  R_improve_PER_PBER  T_save_ER_BER  \\\nBeamRider              0.202360            0.551634      -0.789371   \nBreakout              -0.185787            0.223589      -0.329945   \nQbert                  0.049096            0.301625       0.307731   \nSpaceInvaders         -0.382286            0.109305       0.173007   \nMean                  -0.079154            0.296538      -0.159644   \n\n               T_save_PER_PBER  Improvement_ER_BER  Improvement_PER_PBER  \nBeamRider             0.513878            0.285287              0.476032  \nBreakout              0.433744            0.048994             -0.178539  \nQbert                 0.294004            0.041337              0.080461  \nSpaceInvaders         0.181701           -0.127374              0.173793  \nMean                  0.355832            0.062061              0.137937  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>R_improve_ER_BER</th>\n      <th>R_improve_PER_PBER</th>\n      <th>T_save_ER_BER</th>\n      <th>T_save_PER_PBER</th>\n      <th>Improvement_ER_BER</th>\n      <th>Improvement_PER_PBER</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>BeamRider</th>\n      <td>0.202360</td>\n      <td>0.551634</td>\n      <td>-0.789371</td>\n      <td>0.513878</td>\n      <td>0.285287</td>\n      <td>0.476032</td>\n    </tr>\n    <tr>\n      <th>Breakout</th>\n      <td>-0.185787</td>\n      <td>0.223589</td>\n      <td>-0.329945</td>\n      <td>0.433744</td>\n      <td>0.048994</td>\n      <td>-0.178539</td>\n    </tr>\n    <tr>\n      <th>Qbert</th>\n      <td>0.049096</td>\n      <td>0.301625</td>\n      <td>0.307731</td>\n      <td>0.294004</td>\n      <td>0.041337</td>\n      <td>0.080461</td>\n    </tr>\n    <tr>\n      <th>SpaceInvaders</th>\n      <td>-0.382286</td>\n      <td>0.109305</td>\n      <td>0.173007</td>\n      <td>0.181701</td>\n      <td>-0.127374</td>\n      <td>0.173793</td>\n    </tr>\n    <tr>\n      <th>Mean</th>\n      <td>-0.079154</td>\n      <td>0.296538</td>\n      <td>-0.159644</td>\n      <td>0.355832</td>\n      <td>0.062061</td>\n      <td>0.137937</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv(data_path)\n",
    "n = 10\n",
    "data_df[\"time(h)\"] = data_df[\"time_total_s\"] / 3600\n",
    "window_size_hours = 0.25\n",
    "# 2. Define helper functions\n",
    "\n",
    "def apply_offsets_to_data(data):\n",
    "    recommended_offsets = {\n",
    "        'Boxing': 20,\n",
    "        'Pong': 21,\n",
    "        'FishingDerby': 100,\n",
    "        'Frostbite': 5,\n",
    "        'Enduro': 10,\n",
    "        'Q*bert': 5\n",
    "    }\n",
    "    for env, min_reward in data.groupby('env')['episode_reward_mean'].min().items():\n",
    "        if env not in recommended_offsets and min_reward < 0:\n",
    "            recommended_offsets[env] = abs(min_reward) + 1\n",
    "    data['episode_reward_mean_offset'] = data.apply(lambda row: row['episode_reward_mean'] + recommended_offsets.get(row['env'], 0), axis=1)\n",
    "    return data\n",
    "\n",
    "def compute_interval_mean_with_env(env_data):\n",
    "    env_name = env_data['env'].iloc[0]\n",
    "    bins = list(np.arange(0, env_data[\"time(h)\"].max() + window_size_hours, window_size_hours))\n",
    "    env_data[\"time_interval\"] = pd.cut(env_data[\"time(h)\"], bins, labels=bins[:-1], right=False)\n",
    "    numeric_cols = env_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    interval_data = env_data.groupby('time_interval')[numeric_cols].mean().reset_index()\n",
    "    interval_data['env'] = env_name\n",
    "    return interval_data\n",
    "\n",
    "def compute_avg_reward_last_episodes(env_name, data1, data2, n):\n",
    "    last_rewards1 = data1[data1['env'] == env_name]['episode_reward_mean'].tail(n).values\n",
    "    last_rewards2 = data2[data2['env'] == env_name]['episode_reward_mean'].tail(n).values\n",
    "    avg_reward1 = last_rewards1.mean()\n",
    "    avg_reward2 = last_rewards2.mean()\n",
    "    return avg_reward1, avg_reward2\n",
    "\n",
    "def compute_T_save_for_env(env_name, data1, data2):\n",
    "    rewards1 = data1[data1['env'] == env_name]['episode_reward_mean'].values\n",
    "    rewards2 = data2[data2['env'] == env_name]['episode_reward_mean'].values\n",
    "    if len(rewards1) == 0 or len(rewards2) == 0:\n",
    "        return None\n",
    "    times1 = data1[data1['env'] == env_name]['time(h)'].values\n",
    "    times2 = data2[data2['env'] == env_name]['time(h)'].values\n",
    "    time_diff_ratios = []\n",
    "    max_reward = max(rewards1.max(), rewards2.max())\n",
    "    reward_samples = np.linspace(0, max_reward, 51)\n",
    "    for target_reward in reward_samples:\n",
    "        time1 = next((times1[i] for i, r in enumerate(rewards1) if r >= target_reward), None)\n",
    "        time2 = next((times2[i] for i, r in enumerate(rewards2) if r >= target_reward), None)\n",
    "        if time1 is not None and time2 is not None:\n",
    "            time_diff_ratio = (time1 - time2) / time1\n",
    "            time_diff_ratios.append(time_diff_ratio)\n",
    "    return np.median(time_diff_ratios) if time_diff_ratios else None\n",
    "\n",
    "# 3. Main data processing and analysis steps\n",
    "\n",
    "# Sort data by time\n",
    "data_df_sorted = data_df.sort_values(by=[\"env\", \"time(h)\"])\n",
    "\n",
    "# Split sorted data for ER vs. BER and PER vs. PBER\n",
    "er_data_sorted = data_df_sorted[data_df_sorted['buffer'] == 'ER']\n",
    "ber_data_sorted = data_df_sorted[data_df_sorted['buffer'] == 'BER']\n",
    "per_data_sorted = data_df_sorted[data_df_sorted['buffer'] == 'PER']\n",
    "pber_data_sorted = data_df_sorted[data_df_sorted['buffer'] == 'PBER']\n",
    "\n",
    "# Apply offsets to the data\n",
    "data_df_sorted = apply_offsets_to_data(data_df_sorted)\n",
    "\n",
    "# Compute interval mean for each buffer type and environment using sorted data\n",
    "er_interval_data_sorted = pd.concat([compute_interval_mean_with_env(er_data_sorted[er_data_sorted['env'] == env]) for env in er_data_sorted['env'].unique()])\n",
    "ber_interval_data_sorted = pd.concat([compute_interval_mean_with_env(ber_data_sorted[ber_data_sorted['env'] == env]) for env in ber_data_sorted['env'].unique()])\n",
    "per_interval_data_sorted = pd.concat([compute_interval_mean_with_env(per_data_sorted[per_data_sorted['env'] == env]) for env in per_data_sorted['env'].unique()])\n",
    "pber_interval_data_sorted = pd.concat([compute_interval_mean_with_env(pber_data_sorted[pber_data_sorted['env'] == env]) for env in pber_data_sorted['env'].unique()])\n",
    "\n",
    "# Compute metrics for ER vs. BER and PER vs. PBER using sorted data\n",
    "results_sorted = []\n",
    "for env in data_df_sorted['env'].unique():\n",
    "    R_improve_er_ber = (ber_interval_data_sorted[ber_interval_data_sorted['env'] == env][\"episode_reward_mean\"].median() - er_interval_data_sorted[er_interval_data_sorted['env'] == env][\"episode_reward_mean\"].median()) / ber_interval_data_sorted[ber_interval_data_sorted['env'] == env][\"episode_reward_mean\"].median()\n",
    "    R_improve_per_pber = (pber_interval_data_sorted[pber_interval_data_sorted['env'] == env][\"episode_reward_mean\"].median() - per_interval_data_sorted[per_interval_data_sorted['env'] == env][\"episode_reward_mean\"].median()) / pber_interval_data_sorted[pber_interval_data_sorted['env'] == env][\"episode_reward_mean\"].median()\n",
    "\n",
    "    \n",
    "    T_save_er_ber = compute_T_save_for_env(env, er_data_sorted, ber_data_sorted)\n",
    "    T_save_per_pber = compute_T_save_for_env(env, per_data_sorted, pber_data_sorted)\n",
    "    \n",
    "    avg_er_reward, avg_ber_reward = compute_avg_reward_last_episodes(env, er_data_sorted, ber_data_sorted, n)\n",
    "    avg_per_reward, avg_pber_reward = compute_avg_reward_last_episodes(env, per_data_sorted, pber_data_sorted, n)\n",
    "    \n",
    "    improvement_percentage_er_ber = ((avg_ber_reward - avg_er_reward) / avg_er_reward)  if avg_er_reward != 0 else np.nan\n",
    "    improvement_percentage_per_pber = ((avg_pber_reward - avg_per_reward) / avg_per_reward)  if avg_per_reward != 0 else np.nan\n",
    "    \n",
    "    results_sorted.append({\n",
    "        'Environment': env,\n",
    "        'R_improve_ER_BER': R_improve_er_ber,\n",
    "        'R_improve_PER_PBER': R_improve_per_pber,\n",
    "        'T_save_ER_BER': T_save_er_ber,\n",
    "        'T_save_PER_PBER': T_save_per_pber,\n",
    "        'Improvement_ER_BER': improvement_percentage_er_ber,\n",
    "        'Improvement_PER_PBER': improvement_percentage_per_pber\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame and compute mean values\n",
    "result_df_sorted = pd.DataFrame(results_sorted).set_index('Environment')\n",
    "mean_values_sorted = result_df_sorted.mean(axis=0, skipna=True)\n",
    "mean_values_sorted.name = \"Mean\"\n",
    "result_df_sorted = pd.concat([result_df_sorted, pd.DataFrame(mean_values_sorted).T])\n",
    "\n",
    "result_df_sorted"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-13T11:39:55.241518200Z",
     "start_time": "2023-08-13T11:39:54.795964300Z"
    }
   },
   "id": "5085b1472ee939"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lcccccc}\n",
      "\\toprule\n",
      " & \\multicolumn{3}{c}{ER vs BER} & \\multicolumn{3}{c}{PER vs PBER} \\\\ \\cmidrule(lr){2-4} \\cmidrule(lr){5-7}\n",
      " & $R_{improve}$ & $T_{save}$ & $Improvement_{ER}$ & $R_{improve}$ & $T_{save}$ & $Improvement_{PER}$ \\\\ \\midrule\n",
      "BeamRider & 0.2024 & -0.7894 & 0.2853 & 0.5516 & 0.5139 & 0.4760 \\\\ \n",
      "Breakout & -0.1858 & -0.3299 & 0.0490 & 0.2236 & 0.4337 & -0.1785 \\\\ \n",
      "Qbert & 0.0491 & 0.3077 & 0.0413 & 0.3016 & 0.2940 & 0.0805 \\\\ \n",
      "SpaceInvaders & -0.3823 & 0.1730 & -0.1274 & 0.1093 & 0.1817 & 0.1738 \\\\ \n",
      "Mean & -0.0792 & -0.1596 & 0.0621 & 0.2965 & 0.3558 & 0.1379 \\\\ \n",
      "\\bottomrule\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "def generate_refined_combined_latex_table(df1, df2):\n",
    "    header = \"\\\\begin{tabular}{\" + \"l\" + \"c\"*df1.shape[1] + \"c\"*df2.shape[1] + \"}\\n\\\\toprule\"\n",
    "    \n",
    "    # Headers with formatted names\n",
    "    column_names_1 = [\"$\" + col.split(\"_\")[0] + \"_{\" + col.split(\"_\")[1] + \"}$\" for col in df1.columns]\n",
    "    column_names_2 = [\"$\" + col.split(\"_\")[0] + \"_{\" + col.split(\"_\")[1] + \"}$\" for col in df2.columns]\n",
    "    combined_columns = column_names_1 + column_names_2\n",
    "    \n",
    "    rows = [\" & \\\\multicolumn{3}{c}{ER vs BER} & \\\\multicolumn{3}{c}{PER vs PBER} \\\\\\\\ \\\\cmidrule(lr){2-4} \\\\cmidrule(lr){5-7}\"]\n",
    "    rows.append(\" & \".join([\"\"] + combined_columns) + \" \\\\\\ \\midrule\")\n",
    "    \n",
    "    for index, (row1, row2) in enumerate(zip(df1.iterrows(), df2.iterrows())):\n",
    "        combined_row = list(row1[1].values) + list(row2[1].values)\n",
    "        rows.append(\"{} & {}\".format(row1[0], \" & \".join([\"{:.4f}\".format(val) for val in combined_row])) + \" \\\\\\ \")\n",
    "    \n",
    "    footer = \"\\\\bottomrule\\n\\\\end{tabular}\"\n",
    "    return header + \"\\n\" + \"\\n\".join(rows) + \"\\n\" + footer\n",
    "# Splitting the dataframe into two: ER vs BER and PER vs PBER\n",
    "\n",
    "columns_er_ber = [\"R_improve_ER_BER\", \"T_save_ER_BER\", \"Improvement_ER_BER\"]\n",
    "columns_per_pber = [\"R_improve_PER_PBER\", \"T_save_PER_PBER\", \"Improvement_PER_PBER\"]\n",
    "\n",
    "df_er_ber = result_df_sorted[columns_er_ber]\n",
    "df_per_pber = result_df_sorted[columns_per_pber]\n",
    "combined_latex_table = generate_refined_combined_latex_table(df_er_ber, df_per_pber)\n",
    "print(combined_latex_table)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-13T11:39:55.242516300Z",
     "start_time": "2023-08-13T11:39:55.240516600Z"
    }
   },
   "id": "18bf8191b5396267"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-13T11:39:55.248536400Z",
     "start_time": "2023-08-13T11:39:55.245517900Z"
    }
   },
   "id": "b874c5a060ca37fd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpc",
   "language": "python",
   "name": "hpc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
